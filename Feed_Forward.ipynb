{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f535ffdd",
   "metadata": {},
   "source": [
    "# Question:\n",
    "\n",
    "\n",
    "- Implement the **forward propagation** for a **two hidden layer** network for m-samples, n-features as we discussed in class. Initialize the weights randomly.\n",
    "\n",
    "- Use the data from the previous labs like logistic regression. \n",
    "\n",
    "\n",
    "- You can choose the number of neurons in the hidden layer and use sigmoid activation function.\n",
    "\n",
    "\n",
    "- Report the **evaluation metrics** for the network. \n",
    "\n",
    "\n",
    "- Also use other non-linear activation functions like **ReLU and Tanh.**\n",
    "\n",
    "\n",
    "- Report the loss using both **MSE** and **Cross Entropy.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c40c2",
   "metadata": {},
   "source": [
    "# Step-1: Load The Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bade10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Logistic_regression_ls.csv\")\n",
    "\n",
    "# Extract input features (x1 and x2) and output label (y)\n",
    "X = data[['x1', 'x2']].values\n",
    "y = data['label'].values.reshape(-1, 1)  # Reshape to ensure proper matrix shape\n",
    "\n",
    "# Number of samples and features\n",
    "m, n = X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fca31",
   "metadata": {},
   "source": [
    "# Fixing the number of neurons in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225e7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in each layer\n",
    "n_input = n\n",
    "n_hidden1 = 4  #  First hidden layer\n",
    "n_hidden2 = 3  #  Second hidden layer\n",
    "n_output = 1   # Output layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e896a5",
   "metadata": {},
   "source": [
    " # Step-2: Initialize weights and Bias randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb89f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights randomly\n",
    "np.random.seed(1)  # For reproducibility\n",
    "W1 = np.random.randn(n_input, n_hidden1)\n",
    "W2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "W3 = np.random.randn(n_hidden2, n_output)\n",
    "\n",
    "# Initialize bias randomly\n",
    "b1 = np.zeros((n_hidden1, 1))\n",
    "b2 = np.zeros((n_hidden2, 1))\n",
    "b3 = np.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb2873",
   "metadata": {},
   "source": [
    "# Step-3: Define the evaluation metrics like Precision, Recall, f1_score, Accuracy for the network. (Taken from the previous Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d84457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation Metrics\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\" Precision is the ratio of correctly predicted positive observations\n",
    "        to the total predicted positive observations.\n",
    "    \"\"\"\n",
    "    \n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))  # True Positives\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))  # False Positives\n",
    "    return TP / (TP + FP + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\" Recall (Sensitivity) is the ratio of correctly predicted positive observations\n",
    "        to the all observations in actual class.\n",
    "    \"\"\"\n",
    "    \n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))  # True Positives\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))  # False Negatives\n",
    "    return TP / (TP + FN + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\" F1 Score is the weighted average of Precision and Recall. Therefore, \n",
    "       this score takes both false positives and false negatives into account.\n",
    "    \"\"\"\n",
    "    \n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\" Accuracy  is simply a ratio ofcorrectly predicted observation to the total observations. \"\"\"\n",
    "   \n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    return correct / len(y_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea618c5c",
   "metadata": {},
   "source": [
    "# Define the activation functions like Sigmoid, Relu, Tanh function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6fae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c9dd5",
   "metadata": {},
   "source": [
    "# Implement the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b87cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, W2, W3, activation_func):\n",
    "    # Input to first hidden layer\n",
    "    Z1 = np.dot(X, W1) + b1.T\n",
    "    A1 = activation_func(Z1)\n",
    "    \n",
    "    # First hidden layer to second hidden layer\n",
    "    Z2 = np.dot(A1, W2) + b2.T\n",
    "    A2 = activation_func(Z2)\n",
    "    \n",
    "    # Second hidden layer to output layer\n",
    "    Z3 = np.dot(A2, W3) + b3.T\n",
    "    y_pred = sigmoid(Z3)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9f2ec",
   "metadata": {},
   "source": [
    "# Step-4: Evaluation metrics for Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07b79ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Activation:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.499999999999\n",
      "Recall: 0.9999999999960001\n",
      "F1 Score: 0.6666666662204445\n"
     ]
    }
   ],
   "source": [
    "# Use sigmoid activation function for all layers\n",
    "predictions_sigmoid = forward_propagation(X, W1, W2, W3, sigmoid)\n",
    "binary_predictions_sigmoid = (predictions_sigmoid > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate evaluation metrics with sigmoid activation\n",
    "acc_sigmoid = accuracy(y.flatten(), binary_predictions_sigmoid)\n",
    "prec_sigmoid = precision(y.flatten(), binary_predictions_sigmoid)\n",
    "rec_sigmoid = recall(y.flatten(), binary_predictions_sigmoid)\n",
    "f1_sigmoid = f1_score(y.flatten(), binary_predictions_sigmoid)\n",
    "\n",
    "print(\"Sigmoid Activation:\")\n",
    "print(\"Accuracy:\", acc_sigmoid)\n",
    "print(\"Precision:\", prec_sigmoid)\n",
    "print(\"Recall:\", rec_sigmoid)\n",
    "print(\"F1 Score:\", f1_sigmoid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab38d5",
   "metadata": {},
   "source": [
    "# Step-5: Evaluation metrics for Relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2bcb40a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU Activation:\n",
      "Accuracy: 0.624\n",
      "Precision: 0.5774999999985563\n",
      "Recall: 0.923999999996304\n",
      "F1 Score: 0.7107692302936711\n"
     ]
    }
   ],
   "source": [
    "# Use ReLU activation function for all layers\n",
    "predictions_relu = forward_propagation(X, W1, W2, W3, relu)\n",
    "binary_predictions_relu = (predictions_relu > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate evaluation metrics with ReLU activation\n",
    "acc_relu = accuracy(y.flatten(), binary_predictions_relu)\n",
    "prec_relu = precision(y.flatten(), binary_predictions_relu)\n",
    "rec_relu = recall(y.flatten(), binary_predictions_relu)\n",
    "f1_relu = f1_score(y.flatten(), binary_predictions_relu)\n",
    "\n",
    "print(\"ReLU Activation:\")\n",
    "print(\"Accuracy:\", acc_relu)\n",
    "print(\"Precision:\", prec_relu)\n",
    "print(\"Recall:\", rec_relu)\n",
    "print(\"F1 Score:\", f1_relu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50ee4a",
   "metadata": {},
   "source": [
    "# Evaluation metrics for Tanh function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e30d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanh Activation:\n",
      "Accuracy: 0.458\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Use Tanh activation function for all layers\n",
    "predictions_tanh = forward_propagation(X, W1, W2, W3, tanh)\n",
    "binary_predictions_tanh = (predictions_tanh > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate evaluation metrics with Tanh activation\n",
    "acc_tanh = accuracy(y.flatten(), binary_predictions_tanh)\n",
    "prec_tanh = precision(y.flatten(), binary_predictions_tanh)\n",
    "rec_tanh = recall(y.flatten(), binary_predictions_tanh)\n",
    "f1_tanh = f1_score(y.flatten(), binary_predictions_tanh)\n",
    "\n",
    "print(\"Tanh Activation:\")\n",
    "print(\"Accuracy:\", acc_tanh)\n",
    "print(\"Precision:\", prec_tanh)\n",
    "print(\"Recall:\", rec_tanh)\n",
    "print(\"F1 Score:\", f1_tanh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245c9ad",
   "metadata": {},
   "source": [
    "# Step-6: Defining the  MSE and Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8454043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error (MSE) loss function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Cross-Entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba9ca0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss Sigmoid: 0.26406765187116465\n",
      "Cross-Entropy Loss Sigmoid: 0.7217428874396683\n",
      "\n",
      "MSE Loss Relu: 0.30929875206989166\n",
      "Cross-Entropy Loss Relu: 1.1338787254426874\n",
      "\n",
      "MSE Loss Tanh: 0.35162398774984305\n",
      "Cross-Entropy Loss Tanh: 0.9352836417105388\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For sigmoid activation\n",
    "# Calculate MSE loss\n",
    "mse_loss_sigmoid = mse_loss(y, predictions_sigmoid)\n",
    "print(f\"MSE Loss Sigmoid: {mse_loss_sigmoid}\")\n",
    "\n",
    "# Calculate Cross-Entropy loss\n",
    "cross_entropy_loss_sigmoid = cross_entropy_loss(y, predictions_sigmoid)\n",
    "print(f\"Cross-Entropy Loss Sigmoid: {cross_entropy_loss_sigmoid}\")\n",
    "\n",
    "\n",
    "# For Relu acgtivation\n",
    "# Calculate MSE loss\n",
    "mse_loss_relu = mse_loss(y, predictions_relu)\n",
    "print(f\"\\nMSE Loss Relu: {mse_loss_relu}\")\n",
    "\n",
    "# Calculate Cross-Entropy loss\n",
    "cross_entropy_loss_relu= cross_entropy_loss(y, predictions_relu)\n",
    "print(f\"Cross-Entropy Loss Relu: {cross_entropy_loss_relu}\")\n",
    "\n",
    "\n",
    "# For Tanh activation\n",
    "# Calculate MSE loss\n",
    "mse_loss_tanh = mse_loss(y, predictions_tanh)\n",
    "print(f\"\\nMSE Loss Tanh: {mse_loss_tanh}\")\n",
    "\n",
    "# Calculate Cross-Entropy loss\n",
    "cross_entropy_loss_tanh = cross_entropy_loss(y, predictions_tanh)\n",
    "print(f\"Cross-Entropy Loss Tanh: {cross_entropy_loss_tanh}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68601f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a6b770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
